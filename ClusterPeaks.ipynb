{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T20:11:20.676277Z",
     "start_time": "2021-02-15T20:11:20.660434Z"
    }
   },
   "source": [
    "### Clustering the TFs\n",
    "The code below is to use the peak detector to find the peaks and valleys. Afterward cluster the TFs based on the number of peaks and valleys detected. Output will be folders of TFs based on 'peaks_valleys' and the associated TF barcodes. Also included in output folder is the metaplot histogram and heatmap of all TFs that falled into each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T17:32:44.357175Z",
     "start_time": "2021-02-24T17:32:44.281517Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def read_mds_file(file_path):\n",
    "    \"\"\"Read a csv file containg MDS data into two dimensional array (list of lists)\"\"\"\n",
    "    ids = []\n",
    "    data = []\n",
    "    with open(file_path) as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        i = 0 \n",
    "        for row in readCSV:\n",
    "            if i > 0: # skip the header row\n",
    "                data.append(list(map(float, row[1:])))\n",
    "                ids.append(row[0])\n",
    "            i += 1\n",
    "    return ids, data\n",
    "\n",
    "def read_sim_file(file_path):\n",
    "    \"\"\"Read a txt file containg MDS data into two dimensional array (list of lists)\"\"\"\n",
    "    ids = []\n",
    "    data = []\n",
    "    with open (file_path, 'r') as txtfile:\n",
    "        readTxt = csv.reader(txtfile, delimiter=';')\n",
    "        i = 0 \n",
    "        for row in readTxt:\n",
    "            if i > 0: # skip the header row\n",
    "                data.append(list(map(float, row[1:])))\n",
    "                ids.append(row[0])\n",
    "            i += 1\n",
    "    return ids, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T17:35:27.981238Z",
     "start_time": "2021-02-24T17:32:45.455361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sb\n",
    "import scipy.signal\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.signal import lfilter\n",
    "from statistics import *\n",
    "from itertools import groupby\n",
    "\n",
    "\n",
    "def _smooth_data(raw_data_row):\n",
    "    \"\"\"smooth the data to prepare for usage in a peak finding algorythm\"\"\"\n",
    "    # data_row_tmp0 = data_row\n",
    "    def chunks1(lst, n):\n",
    "        \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "    def chunks2(lst, n):\n",
    "        \"\"\"rolling cluster buckets\"\"\"\n",
    "        for i in range(0, len(lst) - n, 1):\n",
    "            yield lst[i:i + n]\n",
    "    \n",
    "    bucketed_data_row = []\n",
    "    smoothed_data_row = []\n",
    "    \n",
    "    #break data into buckets of 5 units each\n",
    "    #[[0,5], [6, 10], [11,15], ...]\n",
    "    #get the sum of each bucket\n",
    "    #generate new dataset with bucketed data\n",
    "    #this allows us to eccentuate peak data and create larger gaps between signal and noise\n",
    "    for i in chunks1(raw_data_row, 10):\n",
    "        bucketed_data_row.append(sum(i))\n",
    "        \n",
    "    #generate rolling buckets\n",
    "    #[[0,n], [1,n+1], [2, n+2], ...]\n",
    "    #get the mean of each subbucket\n",
    "    #this smooths out noisy peaks and imporoves detection rates\n",
    "    #this lets us filter out 1 unit wide 'peaks' in the middle of noise\n",
    "    #this also makes real peaks easier to detect by smoothing out 'twin tower' peaks\n",
    "    for i in chunks2(bucketed_data_row, 10):\n",
    "        smoothed_data_row.append(mean(i))\n",
    "        \n",
    "    return smoothed_data_row\n",
    "\n",
    "\n",
    "def _plot_raw_data(raw_data_row, image_path):\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,3)\n",
    "    plt.plot(np.array(raw_data_row))\n",
    "    plt.savefig(image_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_heatmap(data, image_path):\n",
    "    #plt.rcParams[\"figure.figsize\"] = (10,3)\n",
    "    heat_map = sb.heatmap([data,], cmap=\"YlGnBu\")\n",
    "    plt.savefig(image_path)\n",
    "    plt.close()\n",
    "\n",
    "# def _plot_peak_data():\n",
    "#     meanArr = np.array([my] * len(y))\n",
    "#     medianArr = np.array([mdy] * len(y))\n",
    "#     plt.rcParams[\"figure.figsize\"] = (10,3)\n",
    "#     plt.plot(y);\n",
    "#     plt.plot(meanArr, color=\"red\")\n",
    "#     plt.plot(medianArr, color=\"red\", linestyle='dashed')\n",
    "#     plt.plot(meanArr + sy15, color=\"orange\")\n",
    "#     plt.plot(meanArr - sy15, color=\"orange\")\n",
    "\n",
    "#========================\n",
    "\n",
    "def find_specified_extrema(rowNum, smoothed_data, peakScan, troughScan):\n",
    "    # plt.rcParams[\"figure.figsize\"] = (10,3)\n",
    "    # plt.plot(np.array(smoothed_data[rowNum]))\n",
    "    # plt.show()\n",
    "    \n",
    "    peaks = []\n",
    "    troughs = []\n",
    "    ymax = max(smoothed_data)\n",
    "    my = mean(smoothed_data)\n",
    "    mdy = median(smoothed_data)\n",
    "    sy = stdev(smoothed_data)\n",
    "    sy15 = sy * 1.5\n",
    "    y = np.array(smoothed_data)\n",
    "    \n",
    "    #invert data before finding peaks\n",
    "    peak_properties = None\n",
    "    trough_properties = None\n",
    "    if peakScan == True:\n",
    "        #returns indexes of y that are detected as peaks\n",
    "        peaks, peak_properties = find_peaks(y, height=my+sy15, distance = 20, prominence=4, width=3)\n",
    "        # plt.plot(peaks, y[peaks], \"xm\");\n",
    "\n",
    "    if troughScan == True:\n",
    "        yinv = ymax - y\n",
    "        myInv = ymax - my\n",
    "        troughs, trough_properties = find_peaks(yinv, height=myInv+sy15, distance = 20, prominence=4, width=3)\n",
    "        # plt.plot(troughs, y[troughs], \"xm\");\n",
    "\n",
    "    # plt.show()\n",
    "    \n",
    "    return len(peaks), len(troughs), peak_properties, trough_properties\n",
    "\n",
    "    \n",
    "#dynamically switch between peak/trough detection\n",
    "#uses stddev outlier detection to determine if there is a substantial negative peak\n",
    "#if n points are less than mean(y) - stddev(y), then the histo will be treated asna trough\n",
    "def find_extrema(raw_data_row):\n",
    "    smoothed_data = _smooth_data(raw_data_row)\n",
    "    my = mean(smoothed_data)\n",
    "    sy = stdev(smoothed_data)\n",
    "    sy15 = sy * 1.5\n",
    "    \n",
    "    #generate signal array\n",
    "    #square signal based on outlier status\n",
    "    peakScan = False\n",
    "    troughScan = False\n",
    "    scanThresh = 5\n",
    "    signals = []\n",
    "    for i in smoothed_data:\n",
    "        if i > (my + sy15):\n",
    "            signals.append(1)\n",
    "        elif i < (my - sy15):\n",
    "            signals.append(-1)\n",
    "        else:\n",
    "            signals.append(0)\n",
    "    #group by signal level and get length of continuous signal\n",
    "    #[0, 0, 1, 1, 1, 0, 1, 1] => [(0,2),(1,3),(0,1),(1,2)]\n",
    "    #if there is a long continous signal, then that indicates peak/trough\n",
    "    grouped_L = [(k, sum(1 for i in g)) for k,g in groupby(signals)]\n",
    "    for (sigVal, count) in grouped_L:\n",
    "        if sigVal == 1 and count > scanThresh:\n",
    "            peakScan = True\n",
    "        if sigVal == -1 and count > scanThresh:\n",
    "            troughScan = True\n",
    "    \n",
    "    return find_specified_extrema(rowNum, smoothed_data, peakScan, troughScan)\n",
    "\n",
    "\n",
    "rowNum = 0\n",
    "\n",
    "#from read_mds_file import read_mds_file\n",
    "# ids, raw_data = read_mds_file('/scratch/Shares/dowell/md_score_paper/mdscore_tsv_files/human/recent/SRR1554311_MDS.csv')\n",
    "ids, raw_data = read_sim_file('levandow_simulated_md_scores_5000.txt')\n",
    "peak_bins_file = open('peak_bins.csv', 'w')\n",
    "peak_bins_file.write('peak,trough,tf\\n')\n",
    "    \n",
    "rows_by_counts = {}\n",
    "for i in range(0, len(raw_data)):\n",
    "    raw_data_row = raw_data[i]\n",
    "    # _plot_raw_data(raw_data_row)\n",
    "    pCount, tCount, peak_properties, trough_properties = find_extrema(raw_data_row)\n",
    "    if (pCount, tCount) not in rows_by_counts:\n",
    "        rows_by_counts[(pCount, tCount)] = []\n",
    "    rows_by_counts[(pCount, tCount)].append(i)\n",
    "    id = ids[i].replace('HO_','').replace('_HUMAN.H10MO','')\n",
    "    peak_bins_file.write('{},{},{}\\n'.format(pCount, tCount, id))\n",
    "    \n",
    "peak_bins_file.close()\n",
    "# print(rows_by_counts)\n",
    "\n",
    "for counts, rownums in rows_by_counts.items():\n",
    "    peak_count, trough_count = counts\n",
    "    print(peak_count, trough_count, rownums)\n",
    "    output_folder = 'output/{}_{}/'.format(peak_count, trough_count)\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)  \n",
    "        \n",
    "    \n",
    "    # var to store aggregated for heat map\n",
    "    # should the size of the raw data intialized to zeros\n",
    "    agg_norm_data = [0 for _ in range(len(raw_data[0]))]\n",
    "    for rownum in rownums:\n",
    "        output_filename = '{}{}.png'.format(output_folder,ids[rownum])\n",
    "        output_filename = output_filename.replace('HO_','')\n",
    "        output_filename = output_filename.replace('_HUMAN.H10MO','')\n",
    "        _plot_raw_data(raw_data[rownum], output_filename)\n",
    "        # normalize the raw data\n",
    "        raw_total = sum(raw_data[rownum])\n",
    "        if raw_total > 0:\n",
    "            normalized_raw_data = [v/raw_total for v in raw_data[rownum]]\n",
    "         # add the normalized data to aggregated data\n",
    "        for i, value in enumerate(normalized_raw_data):\n",
    "            agg_norm_data[i] += value\n",
    "    # plot the aggregated normalized data and store in output_folder\n",
    "    _plot_raw_data(agg_norm_data, '{}meta_histogram.png'.format(output_folder))\n",
    "\n",
    "    # plot heat map of the aggregated normalized data and store in output_folder\n",
    "    plot_heatmap(agg_norm_data, '{}meta_heatmap.png'.format(output_folder))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters based on peaks and valleys\n",
    "Output of peak_bins.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T17:37:29.172334Z",
     "start_time": "2021-02-24T17:37:23.185874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique bins 253\n",
      "         peak  trough  tf\n",
      "bin                      \n",
      "100_100     1       1   1\n",
      "102_109     1       1   1\n",
      "10_10       1       1   1\n",
      "12_12       1       1   1\n",
      "13_13       1       1   2\n",
      "...       ...     ...  ..\n",
      "73_74       1       1   1\n",
      "76_77       1       1   1\n",
      "78_79       1       1   1\n",
      "94_94       1       1   1\n",
      "9_9         1       1   1\n",
      "\n",
      "[253 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('peak_bins.csv')\n",
    "df = df.sort_values(by=['peak', 'trough'])\n",
    "df['bin'] = pd.Series(['' for x in range(len(df.index))])\n",
    "\n",
    "for index in df.index:\n",
    "    bin_value = df.peak.astype(str).str.cat(df.trough.astype(str), sep='_')\n",
    "    df.bin = bin_value \n",
    "\n",
    "num_bin = df['bin'].nunique()\n",
    "print(\"number of unique bins\", num_bin)\n",
    "\n",
    "tf_bin = df.groupby('bin').nunique()\n",
    "print(tf_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
